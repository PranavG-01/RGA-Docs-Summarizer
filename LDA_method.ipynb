{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Pranav_RGA_Reports.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 4\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs=80):\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    # all_stopwords.add('application')\n",
    "    # all_stopwords.add('inspection')\n",
    "    # all_stopwords.add('review')\n",
    "    # all_stopwords.add('customer')\n",
    "    # all_stopwords.add('fail')\n",
    "    # all_stopwords.add('engineering')\n",
    "    # all_stopwords.add('returned')\n",
    "    # all_stopwords.add('lasted')\n",
    "    # all_stopwords.add('bearings')\n",
    "    # all_stopwords.add('clutch')\n",
    "    # all_stopwords.add('machine')\n",
    "    # all_stopwords.add('reported')\n",
    "    # all_stopwords.add('clutches')\n",
    "    # all_stopwords.add('set')\n",
    "    # all_stopwords.add('units')\n",
    "    # all_stopwords.add('seizure')\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            token_text = token.text.lower()\n",
    "            if token_text not in all_stopwords and token.is_alpha:\n",
    "                nr_tokens += 1\n",
    "                freqs[token_text] += 1\n",
    "    \n",
    "    return freqs\n",
    "\n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word] = vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "    \n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs=80):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens  = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Number of emails: {nr_docs}\")\n",
    "    print(f\"Number of tokens: {nr_tokens}\")\n",
    "\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "        \n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "    \n",
    "    return docs, corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EFP 308: Evaluation \\n\\nThe returned item was inspected and per the attached Application Engineering review, no \\nmanufacturing anomalies were detected and cause of customer problem is believed to be \\napplication related. Therefore credit cannot be issued for this unit.'\n",
      " 'Inspection only  \\n\\n8 units were returned for inspection to Florence, KY RGA lab.  It was reported two of \\nthese units were removed from in-service machine until seizure occurred, but it wasn’t \\nclear how long the other 6 lasted.  The inspection was to determine any observed \\ndamages that could help identify possible issues relating to why recent returns haven’t \\nlasted to its expected bearing life. Typically, bearing life was reached at minimum of 2 \\nyears.  Lately, some returns have lasted less than 1 year.'\n",
      " 'Chatter, begin to slip or stall and then go \\n\\nThese units have been used many years on a printer ink roller machine application that \\nhasn’t changed its operating condition.  Typically, these clutches operated continuously \\nlasting about 1 year until recently they haven’t lasted very long.  The replacement of its \\n1st set with 2 clutches lasted about 3 months.   A 2nd set of 2 clutches were replaced \\nonce again lasting about 30 days.  On this last set, the customer reported that some of \\nthe cam’s overall length were longer than previously used set.  This issue was \\nsuspected to have caused the problem preventing the clutch’s free rotation.  In keeping \\nthe machine running, a used clutch was re-used, but as a modified assembly by \\nreplacing the longer cams with cams of the same length.  Since then, the modified \\nclutch worked fine in the machine. \\n\\n'\n",
      " 'TFT-15: Nonconformance/Failure Analysis \\n\\nThe part exhibited excessive axial movement and noise within one month of installation. One \\npart was returned for inspection.'\n",
      " 'USFC5000-PN308-C: PTS Quality Inspection – Non Warranty \\n\\nThe parts have been returned for failure analysis due to shaft seizure. Customer has not provided \\nfurther application information.'\n",
      " 'VPS-335 AH: part is not functioning properly \\n\\nThe returned item was inspected and per the attached Application Engineering review, no \\nmanufacturing anomalies were detected and cause of customer complaint is believed to be \\napplication related. Therefore credit cannot be issued for this unit.'\n",
      " 'SRPB 207-4 CV TF RC: Inspection \\n\\nThese bearings were used on a rotating eccentric weight / vibratory machine driven by a \\n3 HP motor at 1800 rpm and reduced to 659 shaft rpm through 2.69 drive ratio.  It ran \\ncontinuously supporting a radial load exposed to a clean environment re-lubed weekly.  \\nThe 3 HP was used to start runtime.  Its horsepower was ¼ to 1/3 of that.  The inner \\nrace sees a static load due to centrifugal force of the rotating eccentric.  Running \\ncentrifugal force on each bearing was 1,520 lbf radial. \\n\\nIt was reported the bearings were installed on 3/11/2021 and ran for 6 months when \\none of the two bearings had its first seizure occurrence that led to a complete \\ndisassembly with unknown total uptime.  The disassembled bearing showed unusual \\nthin marks on its outer ring raceway.  The 2nd bearing rotated with a tight spot as \\ndescribed by the customer.  They would like to know what caused the thin marks on the \\ndisassembled bearing and the tight spot on the other bearing when it rotated.  \\n\\n'\n",
      " 'For inspection/Application Engineering Review \\n\\n']\n",
      "Number of emails: 8\n",
      "Number of tokens: 0\n",
      "Vocab size: 0\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "data = df['Description'].sample(frac=0.1, random_state=10).values\n",
    "print(data)\n",
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(vocab_idx_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 199871.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, num_itr=200):\n",
    "    Z=[]\n",
    "    num_docs = len(corpus)\n",
    "\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Zd = np.random.randint(low=0,high=NUM_TOPICS,size=(len(doc)))\n",
    "        Z.append(Zd)\n",
    "\n",
    "    ndk = np.zeros((num_docs,NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d]==k)\n",
    "    \n",
    "    nkw = np.zeros((NUM_TOPICS,vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i,word in enumerate(doc):\n",
    "            topic=Z[doc_idx][i]\n",
    "            nkw[topic,word] += 1\n",
    "    \n",
    "    nk = np.sum(nkw,axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "\n",
    "    for _ in tqdm(range(num_itr)):\n",
    "        for doc_idx,doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word=doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                ndk[doc_idx,topic] -= 1\n",
    "                nkw[topic,word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx,:] + ALPHA) * (nkw[:,word] + BETA) / (nk[:] + BETA*vocab_size)\n",
    "                topic = random.choices(topic_list,weights=p_z,k=1)[0]\n",
    "\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx,topic] += 1\n",
    "                nkw[topic,word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words: \n",
      "\n",
      "\n",
      "Topic 1 most common words: \n",
      "\n",
      "\n",
      "Topic 2 most common words: \n",
      "\n",
      "\n",
      "Topic 3 most common words: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw/nk.reshape(NUM_TOPICS,1)\n",
    "\n",
    "num_words = 1\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words: \")\n",
    "    \n",
    "    for word in most_common_words:\n",
    "        print(vocab_idx_str[word])\n",
    "    \n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
